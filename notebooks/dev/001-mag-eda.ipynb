{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from datetime import date\n",
    "from fnf.data.mag_orm import Paper, Author, AuthorAffiliation, Affiliation, AffiliationLocation, PaperAuthor, FieldOfStudy, PaperFieldsOfStudy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the configuration file and create a session.\n",
    "db_config = 'postgres+psycopg2://postgres@localhost/disinfo'\n",
    "engine = create_engine(db_config)\n",
    "Session = sessionmaker(engine)\n",
    "s = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MAG data\n",
    "mag = pd.read_sql(s.query(Paper).statement, s.bind)\n",
    "print(f'MAG data shape: {mag.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling\n",
    "Mainly changing `string` to `np.nan` and codes from Microsoft Academic to human-readable labels.\n",
    "\n",
    "### Data decisions\n",
    "* Dropping papers published in 2020 (found 16 instances). The analysis will focus on full years.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 2020 papers\n",
    "mag = mag[mag.year!='2020']\n",
    "\n",
    "# Some columns have null values registered as 'NaN'\n",
    "mag['bibtex_doc_type'] = mag.bibtex_doc_type.replace('NaN', np.nan)\n",
    "mag['publisher'] = mag.publisher.replace('NaN', np.nan)\n",
    "mag['references'] = mag.references.replace('NaN', np.nan)\n",
    "mag['inverted_abstract'] = mag.inverted_abstract.replace('NaN', np.nan)\n",
    "mag['doi'] = mag.doi.replace('NaN', np.nan)\n",
    "\n",
    "# String to list\n",
    "mag['references'] = mag.references.apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else np.nan)\n",
    "\n",
    "# Change the publication and the bibtex document types\n",
    "publication_type_ = {'0':np.nan, \n",
    "                     '1':'Journal article', \n",
    "                     '2':'Patent', \n",
    "                     '3':'Conference paper',\n",
    "                     '4':'Book chapter',\n",
    "                     '5':'Book',\n",
    "                     '6':'Book reference entry', \n",
    "                     '7':'Dataset', \n",
    "                     '8':'Repository'}\n",
    "\n",
    "bibtext_doc_type_ = {'a':'Journal article', 'b':'Book', 'c':'Book chapter', 'p':'Conference paper'}\n",
    "\n",
    "mag['publication_type'] = mag.publication_type.apply(lambda x: publication_type_[x])\n",
    "mag['bibtex_doc_type'] = mag.bibtex_doc_type.apply(lambda x: bibtext_doc_type_[x] if isinstance(x, str) else np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "55% of the DOIs and 63% of the references are missing. When looking only at papers with a DOI, 20.6% of the abstracts and 34% of the references are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Disinfo papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of missing values\n",
    "(mag.isnull().sum() / mag.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicated titles. Investigate this once online\n",
    "mag.title.value_counts()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The absence of a DOI seems to affect the results. As the graph below shows, paper publication peaks in 2017 (left). However, when examining publications with a DOI, paper publication increases through 2018 and 2019 (right). \n",
    "\n",
    "In both graphs, there's a massive jump in paper publication in 2017 and 2018. Could the 2016 elections in the US and Russian interference cause a massive research interest in the field?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,6))\n",
    "\n",
    "# ax1\n",
    "mag.groupby('year').count()['id'].plot(ax=ax1)\n",
    "ax1.set_title('Disinfo papers in a year')\n",
    "ax1.set_ylabel('Raw frequency')\n",
    "ax1.set_xlabel('Year')\n",
    "\n",
    "# ax2\n",
    "mag[~mag.doi.isnull()].groupby('year').count()['id'].plot(ax=ax2)\n",
    "ax2.set_title('Disinfo papers in a year (with DOI)')\n",
    "ax2.set_ylabel('Raw frequency')\n",
    "ax2.set_xlabel('Year')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the publication types, bibtext document types and publishers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(15,6))\n",
    "\n",
    "# ax1\n",
    "mag.publication_type.value_counts().plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('Publication types')\n",
    "ax1.set_ylabel('Raw frequency')\n",
    "\n",
    "# ax2\n",
    "mag.bibtex_doc_type.value_counts().plot(kind='bar', ax=ax2)\n",
    "ax2.set_title('Bibtext document types')\n",
    "ax2.set_ylabel('Raw frequency')\n",
    "\n",
    "# ax3\n",
    "mag.publisher.value_counts()[:5].plot(kind='bar', ax=ax3)\n",
    "ax3.set_title('Publisher')\n",
    "ax3.set_ylabel('Raw frequency')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count how the number of publishers and publication types has changed over time.\n",
    "\n",
    "The data gathered from Microsoft Academic contain **97 patents** and none of them has a DOI (first row of the graph below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(ncols=2, nrows=2, figsize=(15,10))\n",
    "\n",
    "# ax1\n",
    "mag.groupby(['year', 'publication_type']).count()['id'].unstack('publication_type').plot(kind='bar', stacked=True, ax=ax1)\n",
    "ax1.set_title('Disinfo papers by publication type')\n",
    "ax1.set_ylabel('Raw frequency')\n",
    "ax1.set_xlabel('Year')\n",
    "\n",
    "# ax2\n",
    "mag[~mag.doi.isnull()].groupby(['year', 'publication_type']).count()['id'].unstack('publication_type').plot(kind='bar', stacked=True, ax=ax2)\n",
    "ax2.set_title('Disinfo papers by publication type (with DOI)')\n",
    "ax2.set_ylabel('Raw frequency')\n",
    "ax2.set_xlabel('Year')\n",
    "\n",
    "# Find the publishers with most papers on disinfo\n",
    "top_publishers = mag.publisher.value_counts()[:8].index\n",
    "\n",
    "# ax3\n",
    "pub = mag[mag.publisher.isin(top_publishers)].groupby(['year', 'publisher']).count()['id']\n",
    "pub.unstack('publisher').plot(kind='bar', stacked=True, ax=ax3)\n",
    "ax3.set_title('Disinfo papers by publisher (publication type)')\n",
    "ax3.set_ylabel('Raw frequency')\n",
    "ax3.set_xlabel('Year')\n",
    "\n",
    "# ax4\n",
    "pub = mag[(~mag.doi.isnull()) & (mag.publisher.isin(top_publishers))].groupby(['year', 'publisher']).count()['id']\n",
    "pub.unstack('publisher').plot(kind='bar', stacked=True, ax=ax4)\n",
    "ax4.set_title('Disinfo papers by publisher (with DOI)')\n",
    "ax4.set_ylabel('Raw frequency')\n",
    "ax4.set_xlabel('Year')\n",
    "\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Papers with the most citations\n",
    "\n",
    "We find the average number of citations a paper has received through its lifetime:\n",
    "\n",
    "```math\n",
    "CitationsCount / (CurrentYear - PublicationYear)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(nrows=2, figsize=(15,8))\n",
    "\n",
    "# ax1\n",
    "citations = mag.sort_values('citations', ascending=False).head(20)[['citations', 'year']]\n",
    "citations['year'] = citations.year.apply(lambda x: int(x))\n",
    "citations.plot(kind='scatter', x='year', y='citations', ax=ax1)\n",
    "ax1.set_title('Paper citations')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_xticks([int(i) for i in sorted(mag.year.unique())])\n",
    "ax1.set_xticklabels(sorted(mag.year.unique()))\n",
    "\n",
    "\n",
    "# ax1\n",
    "mag['avg_citations'] = [row['citations'] / (2020 - int(row['year'])) for _, row in mag.iterrows()]\n",
    "avg_citations = mag.sort_values('avg_citations', ascending=False).head(20)[['avg_citations', 'year']]\n",
    "avg_citations['year'] = avg_citations.year.apply(lambda x: int(x))\n",
    "avg_citations.plot(kind='scatter', x='year', y='avg_citations', ax=ax2)\n",
    "ax2.set_title('Paper average citations')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_xlabel('Year')\n",
    "ax2.set_xticks([int(i) for i in sorted(mag.year.unique())])\n",
    "ax2.set_xticklabels(sorted(mag.year.unique()))\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_sql(s.query(Author).statement, s.bind)\n",
    "paper_authors = pd.read_sql(s.query(PaperAuthor).statement, s.bind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average number of co-authors in disinfo papers: {paper_authors.groupby('paper_id').count()['author_id'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank authors by the number of papers they have published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_paper_count = pd.DataFrame(paper_authors.groupby('author_id').count()['paper_id']).reset_index()\n",
    "author_paper_count = author_paper_count.rename(index=str, columns={'author_id':'id', 'paper_id':'paper_count'})\n",
    "author_names_with_paper_count = authors.merge(author_paper_count, left_on='id', right_on='id').sort_values('paper_count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(7,5))\n",
    "author_names_with_paper_count[['name', 'paper_count']].head(10).set_index('name').plot(kind='barh', legend=False, ax=ax)\n",
    "ax.set_title('Author rank: Paper count')\n",
    "ax.set_ylabel('Name')\n",
    "ax.set_xlabel('Count')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank authors by the raw frequency and the average number of times they have been cited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge paper_authors with mag to get their citation and avg_citations count\n",
    "paper_authors_citations = paper_authors.merge(mag[['avg_citations', 'citations', 'id']], left_on='paper_id', right_on='id')\n",
    "\n",
    "# Raw citation count\n",
    "author_citations_sum = paper_authors_citations.groupby('author_id')['citations'].sum().reset_index()\n",
    "author_citations_sum = author_citations_sum.rename(index=str, columns={'author_id':'id', 'citations':'citation_count'})\n",
    "author_names_with_citation_count = authors.merge(author_citations_sum, left_on='id', right_on='id').sort_values('citation_count', ascending=False)\n",
    "\n",
    "# Average citation count\n",
    "author_avg_citations_sum = paper_authors_citations.groupby('author_id')['avg_citations'].sum().reset_index()\n",
    "author_avg_citations_sum = author_avg_citations_sum.rename(index=str, columns={'author_id':'id', 'avg_citations':'avg_citation_count'})\n",
    "author_names_with_avg_citation_count = authors.merge(author_avg_citations_sum, left_on='id', right_on='id').sort_values('avg_citation_count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,6))\n",
    "\n",
    "# ax1\n",
    "author_names_with_avg_citation_count[['name', 'avg_citation_count']].head(10).set_index('name').plot(kind='barh', legend=False, ax=ax1)\n",
    "ax1.set_title('Author rank: Average citation count')\n",
    "ax1.set_ylabel('Name')\n",
    "ax1.set_xlabel('Count')\n",
    "\n",
    "# ax2\n",
    "author_names_with_citation_count[['name', 'citation_count']].head(10).set_index('name').plot(kind='barh', legend=False, ax=ax2)\n",
    "ax2.set_title('Author rank: Citation count')\n",
    "ax2.set_ylabel('Name')\n",
    "ax2.set_xlabel('Count')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Affiliations\n",
    "\n",
    "I randomly sampled 300 MAG affiliations and places returned by Google to examine if the matching worked. I found just a handful of weird matches. I will change them post-collection to a sensible match because some of them are quite important (such as Yahoo):\n",
    "* `'ΦΩΤΟΤΥΠΙΚΟ ΚΕΝΤΡΟ heavycopy@yahoo.com'` -> `'yahoo'`\n",
    "* `'City Clinic Γενική κλινική'` -> `'mayo clinic'`\n",
    "* `'Εθνικο Ιδρυμα Ερευνων'` -> `'National Institute for Medical Research (NIMR)'` (Dar es Salaam, Tanzania)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affiliations =  pd.read_sql(s.query(Affiliation).statement, s.bind)\n",
    "author_aff =  pd.read_sql(s.query(AuthorAffiliation).statement, s.bind)\n",
    "location =  pd.read_sql(s.query(AffiliationLocation).statement, s.bind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing the geocode mismatches I mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in location.iterrows():\n",
    "    if row['name'] == 'ΦΩΤΟΤΥΠΙΚΟ ΚΕΝΤΡΟ heavycopy@yahoo.com':\n",
    "        location.loc[index, 'name'] = 'Yahoo'\n",
    "        location.loc[index, 'administrative_area_level_2'] = 'Santa Clara County'\n",
    "        location.loc[index, 'administrative_area_level_1'] = 'California'\n",
    "        location.loc[index, 'lat'] = 37.409758\n",
    "        location.loc[index, 'lng'] = -121.956869\n",
    "        location.loc[index, 'country'] = 'United States'\n",
    "    elif row['name'] == 'City Clinic Γενική κλινική':\n",
    "        location.loc[index, 'name'] = 'Mayo Clinic'\n",
    "        location.loc[index, 'administrative_area_level_2'] = 'Olmsted County'\n",
    "        location.loc[index, 'administrative_area_level_1'] = 'Minnesota'\n",
    "        location.loc[index, 'lat'] = 43.996637\n",
    "        location.loc[index, 'lng'] = -92.469155\n",
    "        location.loc[index, 'country'] = 'United States'\n",
    "    elif row['name'] == 'Εθνικο Ιδρυμα Ερευνων':\n",
    "        location.loc[index, 'name'] = 'National Institute for Medical Research (NIMR)'\n",
    "        location.loc[index, 'administrative_area_level_2'] = 'Kivukoni'\n",
    "        location.loc[index, 'administrative_area_level_1'] = 'Dar es Salaam'\n",
    "        location.loc[index, 'lat'] = -6.800262\n",
    "        location.loc[index, 'lng'] = 39.285133\n",
    "        location.loc[index, 'country'] = 'Tanzania'\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some authors have multiple affiliations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_aff.groupby('author_id').count()['affiliation_id'].sort_values(ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Unique author affiliations: {affiliations.shape[0]}')\n",
    "print(f'% of geocoded affiliations: {(location.shape[0] / affiliations.shape[0]) * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of missing values\n",
    "(location.isnull().sum() / location.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all tables\n",
    "df = paper_authors.merge(author_aff, left_on='author_id', right_on='author_id') \\\n",
    "                  .merge(location, left_on='affiliation_id', right_on='affiliation_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate authors\n",
    "df = df.drop_duplicates('author_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(ncols=2, nrows=2, figsize=(15,10))\n",
    "\n",
    "# ax1\n",
    "df.groupby('country')['author_id'].count().sort_values(ascending=False)[:25].plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('Country')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "# ax2 \n",
    "df.groupby('name')['author_id'].count().sort_values(ascending=False)[:25].plot(kind='bar', ax=ax2)\n",
    "ax2.set_title('Affiliation')\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "# ax3\n",
    "df.groupby('administrative_area_level_2')['author_id'].count().sort_values(ascending=False)[:25].plot(kind='bar', ax=ax3)\n",
    "ax3.set_title('City')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_xlabel('')\n",
    "\n",
    "# ax4\n",
    "df.groupby('administrative_area_level_1')['author_id'].count().sort_values(ascending=False)[:25].plot(kind='bar', ax=ax4)\n",
    "ax4.set_title('Region')\n",
    "ax4.set_xlabel('')\n",
    "\n",
    "f.suptitle('Geography of disinfo research', y=1.02)\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What changes do we observe when examining publications before and after the massive jump of 2017?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the publication year\n",
    "df = df.merge(mag[['id', 'year', 'citations']], left_on='paper_id', right_on='id')\n",
    "\n",
    "# Split the data before/after 2017\n",
    "df_pre_2017 = df[df.year<'2017']\n",
    "df_post_2017 = df[df.year>='2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,8))\n",
    "\n",
    "# ax1\n",
    "df_pre_2017.groupby('name')['paper_id'].count().sort_values(ascending=False)[:25].plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('Institutions with the most papers in disinfo (pre 2017)')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# ax2\n",
    "df_post_2017.groupby('name')['paper_id'].count().sort_values(ascending=False)[:25].plot(kind='bar', ax=ax2)\n",
    "ax2.set_title('Institutions with the most papers in disinfo (post 2017)')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 32% of the institutions appear among the ones with the most disinfo papers both before and after 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shared institutions in the top 25 before and after 2017: {len(set(df_pre_2017.groupby('name')['paper_id'].count().sort_values(ascending=False)[:25].index) & set(df_post_2017.groupby('name')['paper_id'].count().sort_values(ascending=False)[:25].index))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,8))\n",
    "\n",
    "# ax1\n",
    "df_pre_2017.groupby('name')['citations'].sum().sort_values(ascending=False)[:25].plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('Institutions with the most citations in disinfo (pre 2017)')\n",
    "ax1.set_ylabel('Sum')\n",
    "\n",
    "# ax2\n",
    "df_post_2017.groupby('name')['citations'].sum().sort_values(ascending=False)[:25].plot(kind='bar', ax=ax2)\n",
    "ax2.set_title('Institutions with the most citations in disinfo (post 2017)')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fields of study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fos = pd.read_sql(s.query(FieldOfStudy).statement, s.bind)\n",
    "pfos = pd.read_sql(s.query(PaperFieldsOfStudy).statement, s.bind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of unique Fields of Study in disinfo: {fos.id.unique().shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add field of study names, year and doi to papers\n",
    "pfos = pfos.merge(fos, left_on='field_of_study_id', right_on='id')[['paper_id', 'field_of_study_id', 'name', 'id']]\n",
    "pfos = pfos.merge(mag[['id', 'doi', 'year']], left_on='paper_id', right_on='id')\n",
    "\n",
    "# Keep only papers with doi\n",
    "pfos_doi = pfos.dropna(subset=['doi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I examined the usage of fields of study (ie paper keywords) through time. As previously, I checked both all papers and those with a DOI. The latter chunk reports more interesting findings. I will discuss that.\n",
    "\n",
    "Graph `ax4` shows the most used FoS which I would split into two categories: \n",
    "\n",
    "#### Disciplines/Fields\n",
    "\n",
    "The _field_ of **Medicine** is the most used FoS in the dataset. It is not the most used nowadays (it actually stopped being the most used in 2017, right after the US Elections), however, it has been consistently used throughout the timeframe of the analysis. \n",
    "\n",
    "The popularity of most fields has been growing from 2017, as the number of published papers skyrocketed. The case of **Social media** is interesting. No disinfo papers used that term till 2007, about when social media started taking off. **Computer science** took off on 2017 and has been the most used FoS since then. _I need to dig deeper._ For example, **Artificial intelligence** has been growing - what about more granular topics such as DeepFakes?\n",
    "\n",
    "**History** didn't take off in 2017 as the rest of the fields. Nevertheless, it was used x6 times the next year. _Could this signal a new research topic?_\n",
    "\n",
    "Overall, it looks like the growth of the disinformation research is driven by publications related to **Computer science**, **Political science**, **Media studies** and **Social media**. \n",
    "\n",
    "#### Disinfo terms\n",
    "I examined how the usage of the terms I queried Microsoft Academic with has changed over time. **Misinformation** (spreading  false or inaccurate information) is the term that has been historically used, however, **Fake news** became the primary term to describe work in the field from 2017 and onwards. **Disinformation** (spreading false information deliberately to deceive) has the largest growth rate of all, even though it is still the least used among the three terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(ncols=3, nrows=2, figsize=(15,10))\n",
    "\n",
    "# ax1\n",
    "pfos.name.value_counts()[:25].plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('Most used Fields of Study')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_xlabel('Fields of Study')\n",
    "\n",
    "# ax2\n",
    "i = pd.IndexSlice\n",
    "query_terms = [\"Disinformation\", \"Misinformation\", \"Fake news\", \"Factoid\", \"Half-truth\", \"Misinformation effect\"]\n",
    "pfos.groupby(['year', 'name'])['paper_id'].count().loc[i[:, query_terms]].unstack('name').plot(ax=ax2)\n",
    "ax2.set_title('Trending disinfo terms')\n",
    "\n",
    "# ax3\n",
    "i = pd.IndexSlice\n",
    "query_terms = ['Political science', 'Computer science', 'Internet privacy', 'Advertising', 'Medicine', 'Psychology',\n",
    "               'Public relations', 'Social media', 'Sociology', 'Media studies', 'Artificial intelligence', 'History',\n",
    "               'Humanities']\n",
    "pfos.groupby(['year', 'name'])['paper_id'].count().loc[i[:, query_terms]].unstack('name').plot(ax=ax3)\n",
    "ax3.set_title('Trending disinfo fields')\n",
    "\n",
    "# ax4\n",
    "pfos_doi.name.value_counts()[:25].plot(kind='bar', ax=ax4)\n",
    "ax4.set_title('Most used Fields of Study (only with DOI)')\n",
    "ax4.set_ylabel('Count')\n",
    "ax4.set_xlabel('Fields of Study')\n",
    "\n",
    "# ax5\n",
    "i = pd.IndexSlice\n",
    "query_terms = [\"Disinformation\", \"Misinformation\", \"Fake news\", \"Factoid\", \"Misinformation effect\"]\n",
    "pfos_doi.groupby(['year', 'name'])['paper_id'].count().loc[i[:, query_terms]].unstack('name').plot(ax=ax5)\n",
    "ax5.set_title('Trending disinfo terms (only with DOI)')\n",
    "\n",
    "# ax6\n",
    "i = pd.IndexSlice\n",
    "query_terms = ['Political science', 'Computer science', 'Internet privacy', 'Advertising', 'Medicine', 'Psychology',\n",
    "               'Public relations', 'Social media', 'Sociology', 'Media studies', 'Artificial intelligence', 'History',\n",
    "               'Humanities',]\n",
    "pfos_doi.groupby(['year', 'name'])['paper_id'].count().loc[i[:, query_terms]].unstack('name').plot(ax=ax6)\n",
    "ax6.set_title('Trending disinfo fields (only with DOI)')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country level differences - What are the most used FoS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfos_doi_geo = pfos_doi.merge(df[['paper_id', 'country']], left_on='paper_id', right_on='paper_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfos_doi_geo.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 6 countries with the most disinfo publications, 5 of them use FoS related to fields. **China** is the exception, focusing almost exclusively on **Computer science** and **Artificial intelligence**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(ncols=3, nrows=2, figsize=(15,10))\n",
    "\n",
    "# ax1\n",
    "pfos_doi_geo.groupby(['country', 'name'])['paper_id'].count().loc['United States'].sort_values(ascending=False)[:10].plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('United States')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "# ax2\n",
    "pfos_doi_geo.groupby(['country', 'name'])['paper_id'].count().loc['United Kingdom'].sort_values(ascending=False)[:10].plot(kind='bar', ax=ax2)\n",
    "ax2.set_title('United Kingdom')\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "# ax3\n",
    "pfos_doi_geo.groupby(['country', 'name'])['paper_id'].count().loc['Canada'].sort_values(ascending=False)[:10].plot(kind='bar', ax=ax3)\n",
    "ax3.set_title('Canada')\n",
    "ax3.set_xlabel('')\n",
    "\n",
    "# ax4\n",
    "pfos_doi_geo.groupby(['country', 'name'])['paper_id'].count().loc['Australia'].sort_values(ascending=False)[:10].plot(kind='bar', ax=ax4)\n",
    "ax4.set_title('Australia')\n",
    "ax4.set_xlabel('')\n",
    "\n",
    "# ax5\n",
    "pfos_doi_geo.groupby(['country', 'name'])['paper_id'].count().loc['China'].sort_values(ascending=False)[:10].plot(kind='bar', ax=ax5)\n",
    "ax5.set_title('China')\n",
    "ax5.set_xlabel('')\n",
    "\n",
    "# ax6\n",
    "pfos_doi_geo.groupby(['country', 'name'])['paper_id'].count().loc['Brazil'].sort_values(ascending=False)[:10].plot(kind='bar', ax=ax6)\n",
    "ax6.set_title('Brazil')\n",
    "ax6.set_xlabel('')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How has Computer science, Artificial intelligence, Machine learning, Social media, Political science and Internet privacy been used through time in the countries with the most disinfo papers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(nrows=3, figsize=(12,8))\n",
    "\n",
    "# ax1\n",
    "pfos_doi_geo[pfos_doi_geo.name=='Computer science'].groupby(['year', 'country'])['paper_id'].count().unstack('country')[df.groupby('country')['author_id'].count().sort_values(ascending=False).index[:5].values].plot(kind='bar', rot=0, ax=ax1)\n",
    "ax1.set_title('FoS: Computer science')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "# ax2\n",
    "pfos_doi_geo[pfos_doi_geo.name=='Artificial intelligence'].groupby(['year', 'country'])['paper_id'].count().unstack('country')[df.groupby('country')['author_id'].count().sort_values(ascending=False).index[:5].values].plot(kind='bar', rot=0, ax=ax2)\n",
    "ax2.set_title('FoS: Artificial intelligence')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "# ax2\n",
    "pfos_doi_geo[pfos_doi_geo.name=='Machine learning'].groupby(['year', 'country'])['paper_id'].count().unstack('country')[df.groupby('country')['author_id'].count().sort_values(ascending=False).index[:5].values].plot(kind='bar', rot=0, ax=ax3)\n",
    "ax3.set_title('FoS: Machine learning')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_xlabel('')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(nrows=3, figsize=(12,8))\n",
    "\n",
    "# ax1\n",
    "pfos_doi_geo[pfos_doi_geo.name=='Social media'].groupby(['year', 'country'])['paper_id'].count().unstack('country')[df.groupby('country')['author_id'].count().sort_values(ascending=False).index[:5].values].plot(kind='bar', rot=0, ax=ax1)\n",
    "ax1.set_title('FoS: Social media')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "# ax2\n",
    "pfos_doi_geo[pfos_doi_geo.name=='Political science'].groupby(['year', 'country'])['paper_id'].count().unstack('country')[df.groupby('country')['author_id'].count().sort_values(ascending=False).index[:5].values].plot(kind='bar', rot=0, ax=ax2)\n",
    "ax2.set_title('FoS: Political science')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "# ax2\n",
    "pfos_doi_geo[pfos_doi_geo.name=='Internet privacy'].groupby(['year', 'country'])['paper_id'].count().unstack('country')[df.groupby('country')['author_id'].count().sort_values(ascending=False).index[:5].values].plot(kind='bar', rot=0, ax=ax3)\n",
    "ax3.set_title('FoS: Internet privacy')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_xlabel('')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A network of disinfo research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurrence_graph(elements):\n",
    "    # Get all of the unique entries you have\n",
    "    varnames = tuple(sorted(set(itertools.chain(*elements))))\n",
    "\n",
    "    # Get a list of all of the combinations you have\n",
    "    expanded = [tuple(itertools.combinations(d, 2)) for d in elements]\n",
    "    expanded = itertools.chain(*expanded)\n",
    "\n",
    "    # Sort the combinations so that A,B and B,A are treated the same\n",
    "    expanded = [tuple(sorted(d)) for d in expanded]\n",
    "\n",
    "    # count the combinations\n",
    "    return Counter(expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cooccurrence network of fields of study\n",
    "graph = cooccurrence_graph(pfos.groupby('paper_id')['name'].apply(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for k,v in graph.items():\n",
    "    # Keep only edges where the pair has cooccurred more than 5 times\n",
    "    if v > 5:\n",
    "        G.add_edge(k[0], k[1], weight=int(v))\n",
    "    \n",
    "print(f'Nodes: {len(G)}')\n",
    "print(f'Edges: {len(G.edges)}')\n",
    "\n",
    "nx.write_graphml(G, path='/Users/kstathou/Desktop/fnf/data/interim/disinfo_fos.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the network\n",
    "np.random.seed(42)\n",
    "communities = community.best_partition(G, resolution=.61)\n",
    "print(f'Number of communities: {len(set(communities.values()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colours = {0:'#7e1e9c', 1:'#15b01a', 2:'#0343df', 3:'#f97306', 4:'#e50000', 5:'#ffff14'}\n",
    "colours = {0:u'#1f77b4', 1:u'#ff7f0e', 2:u'#2ca02c', 3:u'#d62728', 4:u'#9467bd', \n",
    "           5:u'#8c564b', 6:u'#e377c2', 7:u'#7f7f7f', 8:u'#bcbd22'}#, 9:u'#17becf'}\n",
    "node_list = list(G.nodes())\n",
    "colour_dict = {k:colours[communities[k]] for k in node_list}\n",
    "nx.set_node_attributes(G, colour_dict, 'color')\n",
    "nx.write_graphml(G, path='/Users/kstathou/Desktop/fnf/data/interim/disinfo_fos_coloured_v2.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "import spacy\n",
    "from itertools import chain\n",
    "from spacy_langdetect import LanguageDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted2abstract(obj):\n",
    "    if isinstance(obj, str):\n",
    "        inverted_index = json.loads(obj)['InvertedIndex']\n",
    "        d = {}\n",
    "\n",
    "        for k, v in inverted_index.items():\n",
    "            if len(v)==1:\n",
    "                d[v[0]] = k\n",
    "            else:\n",
    "                for idx in v:\n",
    "                    d[idx] = k\n",
    "        \n",
    "        return ' '.join([v for _, v in OrderedDict(sorted(d.items())).items()])\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag['abstract'] = mag.inverted_abstract.apply(inverted2abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "# ner_labels = ['GPE', 'NORP', 'PERSON', 'ORG', 'LOC', 'FAC', ]\n",
    "ner_labels = ['CARDINAL', 'ORDINAL', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'DATE']\n",
    "def flatten_lists(l):\n",
    "    \"\"\"Unpacks nested lists into one list of elements.\n",
    "\n",
    "    Args:\n",
    "        lst (:obj:`list` of :obj:`list`)\n",
    "\n",
    "    Returns\n",
    "        (list)\n",
    "    \n",
    "    \"\"\"\n",
    "    return [item for sublist in l if sublist for item in sublist if item]\n",
    "\n",
    "def name_entities(text, ner_labels):\n",
    "    doc = nlp(text)\n",
    "    if doc._.language['language'] == 'en':\n",
    "        return [tuple((ent.text, ent.label_)) for ent in doc.ents if ent.label_ not in ner_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d = {}\n",
    "for _, row in mag.dropna(subset=['abstract']).iterrows():\n",
    "    d[row['id']] = name_entities(row['abstract'], ner_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = flatten_lists(list(d.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "entity = []\n",
    "entity_type = []\n",
    "for k, v in d.items():\n",
    "    if v:\n",
    "        for item in v:\n",
    "            ids.append(k)\n",
    "            entity.append(v[0][0])\n",
    "            entity_type.append(v[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common entities in abstracts\n",
    "Counter(ents).most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_entities = pd.DataFrame({'id':ids, 'entity':entity, 'entity_type':entity_type})\n",
    "# abstract_entities = abstract_entities.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_ents = abstract_entities.merge(df[['country', 'paper_id', 'year']], left_on='id', right_on='paper_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_ents[papers_ents.entity=='Chinese'].groupby('country').count()['paper_id'].sort_values(ascending=False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_entities.entity.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords\n",
    "import inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = {}\n",
    "for idx, text in mag.abstract.dropna().iteritems():\n",
    "    if nlp(text)._.language['language'] == 'en':\n",
    "        kw[idx] = keywords.keywords(text, split=True, ratio=.2)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(kw.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lookups import Lookups\n",
    "lookups = Lookups()\n",
    "lookups.add_table(\"lemma_rules\", {\"noun\": [[\"s\", \"\"]]})\n",
    "lemmatizer = Lemmatizer(lookups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = []\n",
    "for tokens in words:\n",
    "    lst = []\n",
    "    for token in tokens:\n",
    "        lst.extend(lemmatizer(token, 'NOUN'))\n",
    "    w.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cooccurrence network of fields of study\n",
    "graph = cooccurrence_graph(words)\n",
    "\n",
    "G = nx.Graph()\n",
    "for k,v in graph.items():\n",
    "    # Keep only edges where the pair has cooccurred more than 5 times\n",
    "    if v > 7:\n",
    "        G.add_edge(k[0], k[1], weight=int(v))\n",
    "    \n",
    "print(f'Nodes: {len(G)}')\n",
    "print(f'Edges: {len(G.edges)}')\n",
    "\n",
    "nx.write_graphml(G, path='/Users/kstathou/Desktop/fnf/data/interim/disinfo_textrank.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cooccurrence network of fields of study\n",
    "graph = cooccurrence_graph(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for k,v in graph.items():\n",
    "    # Keep only edges where the pair has cooccurred more than 5 times\n",
    "    if v > 8:\n",
    "        G.add_edge(k[0], k[1], weight=int(v))\n",
    "    \n",
    "print(f'Nodes: {len(G)}')\n",
    "print(f'Edges: {len(G.edges)}')\n",
    "\n",
    "nx.write_graphml(G, path='/Users/kstathou/Desktop/fnf/data/interim/disinfo_textrank_singular_words.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
