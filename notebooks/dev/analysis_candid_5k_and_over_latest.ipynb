{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade geoplot\n",
    "# !conda install descartes -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import configparser\n",
    "import json\n",
    "import requests\n",
    "config=configparser.ConfigParser()\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "from collections import Counter\n",
    "from floweaver import *\n",
    "from scipy import stats\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "# from contraction import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import geopandas as gpd\n",
    "# import geoplot\n",
    "import pgeocode\n",
    "nomi = pgeocode.Nominatim('gb')\n",
    "import shapely\n",
    "\n",
    "import string\n",
    "punct = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import sentencepiece as spm\n",
    "USE_LITE = \"https://tfhub.dev/google/universal-sentence-encoder-lite/2\"\n",
    "module = hub.Module(USE_LITE)\n",
    "def process_to_IDs_in_sparse_format(sp, documents):\n",
    "    \"\"\"Process documents with the SentencePiece processor. The results have a format\n",
    "    similar to tf.SparseTensor (values, indices, dense_shape).\"\"\"\n",
    "    ids = [sp.EncodeAsIds(x) for x in documents]\n",
    "    max_len = max(len(x) for x in ids)\n",
    "    dense_shape = (len(ids), max_len)\n",
    "    values = [item for sublist in ids for item in sublist]\n",
    "    indices = [[row, col] for row in range(len(ids)) for col in range(len(ids[row]))]\n",
    "    return (values, indices, dense_shape)\n",
    "def docs2vectors(documents, module):\n",
    "    \"\"\"Find the vector representation of a collection of documents using Google's\n",
    "    Universal Sentence Encoder (lite) model.\n",
    "    Args:\n",
    "        documents (:obj:`list` of :obj:`str`): List of raw text documents.\n",
    "    Returns:\n",
    "        doc_embeddings (:obj:`numpy.array` of :obj:`numpy.array` of :obj:`float`): Vector representation of documents.\n",
    "    \"\"\"\n",
    "    input_placeholder = tf.compat.v1.sparse_placeholder(tf.int64, shape=[None, None])\n",
    "    encodings = module(\n",
    "        inputs=dict(\n",
    "            values=input_placeholder.values,\n",
    "            indices=input_placeholder.indices,\n",
    "            dense_shape=input_placeholder.dense_shape))\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        spm_path = sess.run(module(signature=\"spm_path\"))\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.Load(spm_path)\n",
    "        # Preprocess documents\n",
    "        values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, documents)\n",
    "        sess.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer()])\n",
    "        doc_embeddings = sess.run(\n",
    "                                encodings,\n",
    "                                feed_dict={input_placeholder.values: values,\n",
    "                                           input_placeholder.indices: indices,\n",
    "                                           input_placeholder.dense_shape: dense_shape})\n",
    "    return doc_embeddings\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"tensorflow==1.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_5k = pd.read_csv('../../data/interim/df_grants_5k_and_over_df.csv', index_col=[0])\n",
    "# df_under_5k = pd.read_csv('data/df_grants_under_5k_df.csv')\n",
    "\n",
    "df_over_5k_media = pd.read_csv('../../data/interim/grants_5k_and_over_media_all.csv', index_col=[0])\n",
    "# df_under_5k_media = pd.read_csv('data/grants_under_5k_media_all.csv')\n",
    "\n",
    "df_over_5k_gb = pd.read_csv('../../data/interim/grants_5k_and_over_england_filtered_05-12-19.csv', index_col=[0])\n",
    "df_over_5k_gb = df_over_5k_gb[df_over_5k_gb.columns[1:]]\n",
    "# df_under_5k_eng = pd.read_csv('data/grants_under_5k_england_filtered_05-12-19.csv')\n",
    "\n",
    "df_over_5k_gb_media = pd.read_csv('../../data/interim/grants_5k_and_over_england_media_filtered_05-12-19.csv', index_col=[0])\n",
    "# df_under_5k_eng_media = pd.read_csv('data/grants_under_5k_england_media_filtered_05-12-19.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_over_5k))\n",
    "print(len(df_over_5k_media))\n",
    "\n",
    "print(len(df_over_5k_gb))\n",
    "print(len(df_over_5k_gb_media))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Number of Unique Values per field*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_over_5k[col].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in df_over_5k.columns:\n",
    "    print('{}: {}'.format(col,str(df_over_5k[col].is_unique)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df_over_5k.nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Number of Entries: {}'.format(len(df_over_5k)))\n",
    "print('Number of Grant Keys: {}'.format(unique_values[0]))\n",
    "unique_values.plot(kind='barh', fontsize=8, figsize=(5,11))\n",
    "# plt.yticks(ha='right', rotation=45, fontsize=9)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Number of Unique values per Column')\n",
    "plt.ylabel('Column')\n",
    "plt.xlabel('Frequency')\n",
    "plt.grid(alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_5k[(df_over_5k.duplicated(subset='grant_key', keep=False)==True) & (df_over_5k['grant_key'] == 20464956)]['source_file_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_5k['gm_country'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting rid of dups\n",
    "df_over_5k.drop_duplicates(subset = 'grant_key',keep = 'last', inplace=True)\n",
    "df_over_5k_media.drop_duplicates(subset = 'grant_key',keep = 'last', inplace=True)\n",
    "df_over_5k_gb.drop_duplicates(subset = 'grant_key',keep = 'last', inplace=True)\n",
    "df_over_5k_gb_media.drop_duplicates(subset = 'grant_key',keep = 'last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_over_5k))\n",
    "print(len(df_over_5k_media))\n",
    "print(len(df_over_5k_gb))\n",
    "print(len(df_over_5k_gb_media))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Number of Missing Values per Field*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame((df_over_5k.isnull()\n",
    " .sum().sort_values(ascending=False)/df_over_5k.shape[0])).loc['developing_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(df_over_5k.isnull()\n",
    " .sum().sort_values(ascending=False)/df_over_5k.shape[0]).plot(kind='barh', figsize=(5,11))\n",
    "\n",
    "# plt.xticks(ha='right', rotation=45, fontsize=7)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.ylabel('Column')\n",
    "plt.xlabel('Fraction')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Proportion of Missing (Null) Values per Columns')\n",
    "plt.grid(alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: Courier New;\">A significant amount of columns have at least 50% of values missing. Additionally, a handful have at least 90% os data missing. Some columns that may be important in the analysis may not be possible to work with because of this. Columns such as:</span>\n",
    "\n",
    "- recip_country_tran\n",
    "- gm_country\n",
    "- duration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Possible United States Funds*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_5k_gb['gm_country'].isnull().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_5k_gb[(df_over_5k_gb['gm_country'].isnull())]['gm_city'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_5k[(df_over_5k['recip_country'].isnull())]['recip_city'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_5k[(df_over_5k['recip_country'].isnull())]['recip_city'].value_counts()[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:\tCourier New;\">Many funds have cities but not country attached. Can safely assume that majority of these as from the United States. A probability that a city may not be in the U.S. but these are low in frequency.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#assigning U.S./United States\n",
    "df_over_5k['gm_country'].fillna('US', inplace=True)\n",
    "df_over_5k['recip_country'].fillna('US', inplace=True)\n",
    "# df_over_5k_media['gm_country'].fillna('US', inplace=True)\n",
    "# df_over_5k_media['recip_country'].fillna('US', inplace=True)\n",
    "df_over_5k_gb['gm_country'].fillna('US', inplace=True)\n",
    "df_over_5k_gb['recip_country'].fillna('US', inplace=True)\n",
    "# df_over_5k_england_media['gm_country'].fillna('US', inplace=True)\n",
    "# df_over_5k_england_media['recip_country'].fillna('US', inplace=True)\n",
    "\n",
    "df_over_5k['gm_country_tran'].fillna('United States', inplace=True)\n",
    "df_over_5k['recip_country_tran'].fillna('United States', inplace=True)\n",
    "# df_over_5k_media['gm_country_tran'].fillna('United States', inplace=True)\n",
    "# df_over_5k_media['recip_country_tran'].fillna('United States', inplace=True)\n",
    "df_over_5k_gb['gm_country_tran'].fillna('United States', inplace=True)\n",
    "df_over_5k_gb['recip_country_tran'].fillna('United States', inplace=True)\n",
    "# df_over_5k_england_media['gm_country_tran'].fillna('United States', inplace=True)\n",
    "# df_over_5k_england_media['recip_country_tran'].fillna('United States', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So quick overview\n",
    "dataframe uk includes all grants sent to the U.K.! So also includes international GMs and  obvs not all U.K. GMs in the uk df since some are sent outside! Hence for `df_over_5k_all_uk_gms`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_over_5k_all_gb_gms = df_over_5k[df_over_5k['gm_country'] =='GB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_over_5k.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:\tCourier New;\">\n",
    "    Around 20,000 unique values in the \"Amount\" field. The number of grant keys is less than the total amount of entries.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tend to invest in round numbers?**\n",
    "\n",
    "- **Installments of investment? (do duplicates on keys and look at amount)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_under_5k['gm_country'].fillna('US', inplace=True)\n",
    "df_under_5k['recip_country'].fillna('US', inplace=True)\n",
    "\n",
    "df_under_5k['gm_country_tran'].fillna('United States', inplace=True)\n",
    "df_under_5k['recip_country_tran'].fillna('United States', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fnf_keywords = [\n",
    "    'news', 'public', 'interest',\n",
    "    'journalism', 'journalist', 'journalists',\n",
    "    'innovation', 'innovate', 'innovative',\n",
    "    'engagement', 'engage', 'engaging', 'engaged',\n",
    "    'media', 'podcast', 'podcasts', 'digital',\n",
    "    'video', 'videos', 'film', 'films', 'radio',\n",
    "    'print', 'printing', 'newspapers', 'newspaper',\n",
    "    'magazine', 'magazines', 'publishing', 'publisher', 'publishers', 'publish',\n",
    "    'broadcast', 'broadcasting'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eng_list = df_over_5k[(df_over_5k['recip_country'] =='GB')]['recip_city'].value_counts().keys().to_list()\n",
    "eng_under_list = df_under_5k[(df_under_5k['recip_country'] =='GB')]['recip_city'].value_counts().keys().to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('data/fnf_keywords.txt') as f:\n",
    "    fnf_cities = list(literal_eval(f.read()))\n",
    "    \n",
    "\n",
    "# with open('file.txt') as f:\n",
    "#     dictionaries = literal_eval(f.read().strip()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_grants_over_5k_2 = df_over_5k[(df_over_5k['recip_country'] == 'GB') | (df_over_5k['recip_country_tran'] == 'United Kingdom')]\n",
    "df_grants_over_5k_2['recip_name'] = df_grants_over_5k_2['recip_name'].fillna('')\n",
    "df_grants_over_5k_2['description'] = df_grants_over_5k_2['description'].fillna('')\n",
    "df_grants_over_5k_england = df_grants_over_5k_2.loc[(df_grants_over_5k_2['recip_city'].isin(fnf_cities)) | (df_grants_over_5k_2['recip_city'].fillna('') == '')]\n",
    "\n",
    "df_grants_under_5k_2 = df_under_5k[(df_under_5k['recip_country'] == 'GB') | (df_under_5k['recip_country_tran'] == 'United Kingdom')]\n",
    "df_grants_under_5k_2['recip_name'] = df_grants_under_5k_2['recip_name'].fillna('')\n",
    "df_grants_under_5k_2['description'] = df_grants_under_5k_2['description'].fillna('')\n",
    "df_grants_under_5k_england = df_grants_under_5k_2.loc[(df_grants_under_5k_2['recip_city'].isin(fnf_cities)) | (df_grants_under_5k_2['recip_city'].fillna('') == '')]\n",
    "\n",
    "\n",
    "df_grants_under_5k_england['descrip_and_name'] = df_grants_under_5k_england['recip_name'] + ' ' + df_grants_under_5k_england['description']\n",
    "df_grants_over_5k_england['descrip_and_name'] = df_grants_over_5k_england['recip_name'] + ' ' + df_grants_over_5k_england['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_grants_under_5k_england.to_csv('data/grants_under_5k_england_filtered_05-12-19.csv')\n",
    "\n",
    "df_grants_over_5k_england.to_csv('data/grants_5k_and_over_england_filtered_05-12-19.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tokenise to filter out mmedia\n",
    "# df_grants_over_5k_all['tokens'] = df_grants_over_5k_all['descrip_and_name'].apply(lambda x: x.lower()).apply(word_tokenize)\n",
    "# df_grants_under_5k_all['tokens'] = df_grants_under_5k_all['descrip_and_name'].apply(lambda x: x.lower()).apply(word_tokenize)\n",
    "\n",
    "df_grants_under_5k_england['tokens'] = df_grants_under_5k_england['descrip_and_name'].apply(lambda x: x.lower()).apply(word_tokenize)\n",
    "df_grants_over_5k_england['tokens'] = df_grants_over_5k_england['descrip_and_name'].apply(lambda x: x.lower()).apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_grants_under_5k_england_filtered = df_grants_under_5k_england[df_grants_under_5k_england['tokens'].apply(lambda x: any(t in fnf_keywords for t in x)) == True]\n",
    "\n",
    "df_grants_over_5k_england_filtered = df_grants_over_5k_england[df_grants_over_5k_england['tokens'].apply(lambda x: any(t in fnf_keywords for t in x)) == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_grants_over_5k_england_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(df_grants_under_5k_england_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_grants_under_5k_england_filtered.to_csv('data/grants_under_5k_england_media_filtered_05-12-19.csv')\n",
    "df_grants_over_5k_england_filtered.to_csv('data/grants_5k_and_over_england_media_filtered_05-12-19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_grants_over_5k_england_filtered_less = df_grants_over_5k_england_filtered[['grant_key','gm_name', 'gm_country', 'gm_city', 'recip_name',\n",
    "                                       'recip_country', 'recip_city', 'amount', 'yr_issued', 'duration']]\n",
    "\n",
    "df_grants_under_5k_england_filtered_less = df_grants_under_5k_england_filtered[['grant_key','gm_name', 'gm_country', 'gm_city', 'recip_name',\n",
    "                                       'recip_country', 'recip_city', 'amount', 'yr_issued', 'duration']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_grants_under_5k_england_filtered_less.to_csv('data/grants_under_5k_england_media_filtered_lesscols_05-12-19.csv')\n",
    "df_grants_over_5k_england_filtered_less.to_csv('data/grants_5k_and_over_england_media_filtered_lesscols_05-12-19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_over_5k_england_media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_over_5k_england_media = pd.read_csv('data/grants_5k_and_over_england_media_filtered.csv')\n",
    "df_under_5k_england_media = pd.read_csv('data/grants_under_5k_england_media_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_over = df_over_5k_england_media[['grant_key','gm_name', 'gm_country', 'gm_city', 'recip_name',\n",
    "                                       'recip_country', 'recip_city', 'amount', 'yr_issued', 'duration']]\n",
    "common_over = df_grants_over_5k_england_filtered_less.merge(df_over, on = 'grant_key')\n",
    "df_grants_over_5k_england_filtered_less[(~df_grants_over_5k_england_filtered_less.grant_key.isin(common_over.grant_key))].to_csv('data/grants_over_5k_media_england_05_12_19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_under = df_under_5k_england_media[['grant_key','gm_name', 'gm_country', 'gm_city', 'recip_name',\n",
    "                                       'recip_country', 'recip_city', 'amount', 'yr_issued', 'duration']]\n",
    "common_over = df_grants_under_5k_england_filtered_less.merge(df_under, on = 'grant_key')\n",
    "df_grants_under_5k_england_filtered_less[(~df_grants_under_5k_england_filtered_less.grant_key.isin(common_over.grant_key))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(df_grants_over_5k_england_filtered_less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_over_5k_england_media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Grants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Top grant makers & grant recipients (number of projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- GM that have rewarded the most (by number of projects)\n",
    "- Recips that have received the most (by number of projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "figs, ax = plt.subplots(ncols=2, figsize=(12,4.5))\n",
    "df_over_5k['gm_name'].value_counts()[:20].sort_values(ascending=False).plot(kind='barh', fontsize=9, ax=ax[0], color = '#0022f5')\n",
    "df_over_5k['recip_name'].value_counts()[:20].sort_values(ascending=False).plot(kind='barh', fontsize=9, ax=ax[1], color = '#0022f5')\n",
    "# ax[0].grid(alpha=0.1)\n",
    "# ax[1].grid(alpha=0.1)\n",
    "\n",
    "ax[0].set_title('Top Grant Makers', fontsize=9)\n",
    "ax[0].set_xlabel('Number of Grants Awarded')\n",
    "ax[0].invert_yaxis()\n",
    "\n",
    "ax[1].set_title('Top Frequent Recipients of Grants', fontsize=9)\n",
    "ax[1].set_xlabel('Number of Grants Received')\n",
    "ax[1].invert_yaxis()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Top 20 make up {:.2f}% of the project space (out of {} projects)'.format((df_over_5k['gm_name'].value_counts()[:20].sort_values(ascending=False).sum()/len(df_over_5k))*100,len(df_over_5k)))\n",
    "print('Top 20 recipients make up {:.2f}% of the project space (out of {} projects)'.format((df_over_5k['recip_name'].value_counts()[:20].sort_values(ascending=False).sum()/len(df_over_5k))*100, len(df_over_5k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "UK only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "####UK\n",
    "figs, ax = plt.subplots(ncols=2, figsize=(12,4.5))\n",
    "df_over_5k_all_gb_gms['gm_name'].value_counts()[:20].sort_values(ascending=False).plot(kind='barh', fontsize=9, ax=ax[0], color = '#0022f5')\n",
    "df_over_5k_gb['recip_name'].value_counts()[:20].sort_values(ascending=False).plot(kind='barh', fontsize=9, ax=ax[1], color = '#0022f5')\n",
    "# ax[0].grid(alpha=0.1)\n",
    "# ax[1].grid(alpha=0.1)\n",
    "\n",
    "ax[0].set_title('Top Grant Makers', fontsize=9)\n",
    "ax[0].set_xlabel('Number of Grants Awarded')\n",
    "ax[0].invert_yaxis()\n",
    "\n",
    "ax[1].set_title('Top Frequent Recipients of Grants', fontsize=9)\n",
    "ax[1].set_xlabel('Number of Grants Received')\n",
    "ax[1].invert_yaxis()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Top 20 UK make up {:.2f}% of the project space (out of {} projects)'.format((df_over_5k_all_gb_gms['gm_name'].value_counts()[:20].sort_values(ascending=False).sum()/len(df_over_5k))*100,len(df_over_5k)))\n",
    "print('Top 20 UK recipients make up {:.2f}% of the project space (out of {} projects)'.format((df_over_5k_gb['recip_name'].value_counts()[:20].sort_values(ascending=False).sum()/len(df_over_5k))*100, len(df_over_5k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Top grant makers and recipients (total amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "figs, ax = plt.subplots(ncols=2, figsize=(12,4.5))\n",
    "df_over_5k.pivot_table(index=df_over_5k[['gm_name']], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False)[:20].plot(kind='barh', fontsize=9, ax=ax[0], logx=True, color = '#0022f5')\n",
    "df_over_5k.pivot_table(index=df_over_5k[['recip_name']], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False)[:20].plot(kind='barh', fontsize=9, ax=ax[1], logx=True, color = '#0022f5')\n",
    "ax[0].grid(alpha=0.3)\n",
    "ax[1].grid(alpha=0.3)\n",
    "\n",
    "ax[0].set_title('Top Grant Makers')\n",
    "ax[0].set_xlabel('Amount of funding awarded over the years (log)')\n",
    "ax[0].invert_yaxis()\n",
    "\n",
    "ax[1].set_title('Top Frequent Recipients of Grants')\n",
    "ax[1].set_xlabel('Amount of funding received over the years (log)')\n",
    "ax[1].invert_yaxis()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "total_top_20_gm = (df_over_5k.pivot_table(index=df_over_5k[['gm_name']], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False)[:20]).sum()\n",
    "total_top_20_recip = (df_over_5k.pivot_table(index=df_over_5k[['recip_name']], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False)[:20]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "total_funds_gm = df_over_5k.pivot_table(index=df_over_5k[['gm_name']], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False).sum()\n",
    "total_funds_recip = df_over_5k.pivot_table(index=df_over_5k[['recip_name']], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Top 20 funders make up {:.2f}% of total funding globally'.format((total_top_20_gm/total_funds_gm)*100))\n",
    "print('Top 20 recipients recieve {:.2f}% of total funding globally'.format((total_top_20_recip/total_funds_recip)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print('Top 20 make up {:.2f}% of the project space (out of {} projects)'.format((df_over_5k['gm_name'].value_counts()[:20].sort_values(ascending=False).sum()/len(df_over_5k))*100,len(df_over_5k)))\n",
    "# print('Top 20 recipients make up {:.2f}% of the project space (out of {} projects)'.format((df_over_5k['recip_name'].value_counts()[:20].sort_values(ascending=False).sum()/len(df_over_5k))*100, len(df_over_5k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "U.K. only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "###uk \n",
    "figs, ax = plt.subplots(ncols=2, figsize=(12,4.5))\n",
    "\n",
    "df_over_5k_all_gb_gms.pivot_table(index=df_over_5k_all_gb_gms[['gm_name']], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False)[:20].plot(kind='barh', fontsize=9, ax=ax[0], logx=True, color = '#0022f5')\n",
    "df_over_5k_gb.pivot_table(index=df_over_5k[['recip_name']], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False)[:20].plot(kind='barh', fontsize=9, ax=ax[1], logx=True, color = '#0022f5')\n",
    "ax[0].grid(alpha=0.3)\n",
    "ax[1].grid(alpha=0.3)\n",
    "\n",
    "ax[0].set_title('Top Grant Makers')\n",
    "ax[0].set_xlabel('Amount of funding awarded over the years (log)')\n",
    "ax[0].invert_yaxis()\n",
    "\n",
    "ax[1].set_title('Top Frequent Recipients of Grants')\n",
    "ax[1].set_xlabel('Amount of funding received over the years (log)')\n",
    "ax[1].invert_yaxis()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "total_top_20_gm = (df_over_5k_all_gb_gms.pivot_table(index=df_over_5k_all_gb_gms[['gm_name']], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False)[:20]).sum()\n",
    "total_top_20_recip = (df_over_5k_gb.pivot_table(index=df_over_5k_gb[['recip_name']], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False)[:20]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "total_funds_gm = df_over_5k.pivot_table(index=df_over_5k[['gm_name']], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False).sum()\n",
    "total_funds_recip = df_over_5k.pivot_table(index=df_over_5k[['recip_name']], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Top 20 funders make up {:.2f}% of total funding globally'.format((total_top_20_gm/total_funds_gm)*100))\n",
    "print('Top 20 recipients recieve {:.2f}% of total funding globally'.format((total_top_20_recip/total_funds_recip)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Are some of these \"projects\" grant installments\"?**\n",
    "\n",
    "- Collection of recipients (Anon Recip.)\n",
    "\n",
    "- groupby on countries and select some of the countries (top recips/funders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Funders and Recipients Countries (Global)- Number of Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figs, axs = plt.subplots(ncols=2, figsize=(9,4.5))\n",
    "df_over_5k['gm_country_tran'].value_counts()[:10].plot(kind='barh', ax=axs[0], logx=True, color = '#0022f5')\n",
    "df_over_5k['recip_country_tran'].value_counts()[:10].plot(kind='barh', ax=axs[1], logx = True, color = '#0022f5')\n",
    "\n",
    "plt.setp(axs[0].get_xticklabels(), fontsize=8)\n",
    "plt.setp(axs[1].get_xticklabels(), fontsize=8)\n",
    "\n",
    "# axs[0].set_ylim(0, 140000)\n",
    "# axs[1].set_ylim(0, 140000)\n",
    "\n",
    "axs[0].get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n",
    "axs[1].get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n",
    "\n",
    "axs[0].set_xlabel('Number of Projects')\n",
    "axs[0].set_ylabel('Country', fontsize=10)\n",
    "axs[0].set_title('Top 10 Countries with the most funders', fontsize=10)\n",
    "axs[0].grid(alpha=0.1)\n",
    "axs[0].invert_yaxis()\n",
    "\n",
    "axs[1].set_xlabel('Number of Projects')\n",
    "axs[1].set_ylabel('Country', fontsize=10)\n",
    "axs[1].set_title('Top 10 Countries with the most recipients', fontsize=10)\n",
    "axs[1].grid(alpha=0.1)\n",
    "axs[1].invert_yaxis()\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figs/top_countries_no_projs.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 10 make up {:.2f}% of the project space (out of {} projects)'.format((df_over_5k['gm_country_tran'].value_counts()[:10].sort_values(ascending=False).sum()/len(df_over_5k))*100,len(df_over_5k)))\n",
    "print('Top 10 recipients make up {:.2f}% of the project space (out of {} projects)'.format((df_over_5k['recip_country_tran'].value_counts()[:10].sort_values(ascending=False).sum()/len(df_over_5k))*100, len(df_over_5k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-log\n",
    "\n",
    "figs, axs = plt.subplots(ncols=2, figsize=(9,4.5))\n",
    "df_over_5k['gm_country_tran'].value_counts()[:10].plot(kind='barh', ax=axs[0], color = '#0022f5')\n",
    "df_over_5k['recip_country_tran'].value_counts()[:10].plot(kind='barh', ax=axs[1], color = '#0022f5')\n",
    "\n",
    "plt.setp(axs[0].get_xticklabels(), fontsize=8)\n",
    "plt.setp(axs[1].get_xticklabels(), fontsize=8)\n",
    "\n",
    "# axs[0].set_ylim(0, 140000)\n",
    "# axs[1].set_ylim(0, 140000)\n",
    "\n",
    "# axs[0].get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n",
    "# axs[1].get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n",
    "\n",
    "axs[0].set_xlabel('Number of Projects')\n",
    "axs[0].set_ylabel('Country', fontsize=10)\n",
    "axs[0].set_title('Top 10 Countries with the most funders', fontsize=10)\n",
    "axs[0].grid(alpha=0.1)\n",
    "axs[0].invert_yaxis()\n",
    "\n",
    "axs[1].set_xlabel('Number of Projects')\n",
    "axs[1].set_ylabel('Country', fontsize=10)\n",
    "axs[1].set_title('Top 10 Countries with the most recipients', fontsize=10)\n",
    "axs[1].grid(alpha=0.1)\n",
    "axs[1].invert_yaxis()\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figs/top_countries_no_projs.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_over_5k['gm_country_tran'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Top Funders and Recipients Countries (Global)- Amount of Funding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "figs, ax = plt.subplots(ncols=2, figsize=(9,4.5))\n",
    "df_over_5k.pivot_table(index=df_over_5k[['gm_country_tran']], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False)[:10].plot(kind='barh', fontsize=9, ax=ax[0], logx=True, color = 'C2')\n",
    "df_over_5k.pivot_table(index=df_over_5k[['recip_country_tran']], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False)[:10].plot(kind='barh', fontsize=9, ax=ax[1], logx=True, color = 'C2')\n",
    "ax[0].grid(alpha=0.1)\n",
    "ax[1].grid(alpha=0.1)\n",
    "\n",
    "plt.setp(ax[0].get_xticklabels(),fontsize=8)\n",
    "plt.setp(ax[1].get_xticklabels(), fontsize=8)\n",
    "\n",
    "ax[0].set_title('Top Grant Makers Countries')\n",
    "ax[0].set_xlabel('Amount of funding awarded over the years')\n",
    "ax[0].set_ylabel('Country', fontsize=10)\n",
    "ax[0].invert_yaxis()\n",
    "\n",
    "ax[1].set_title('Top Frequent Recipient Countries of Grants')\n",
    "ax[1].set_xlabel('Amount of funding received over the years')\n",
    "ax[1].set_ylabel('Country', fontsize=10)\n",
    "ax[1].invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/top_countries_total_amount.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "total_top_10_gm = (df_over_5k.pivot_table(index=df_over_5k['gm_country_tran'], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False)[:10]).sum()\n",
    "total_top_10_recip = (df_over_5k.pivot_table(index=df_over_5k['recip_country_tran'], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False)[:20]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "total_funds_gm = df_over_5k.pivot_table(index=df_over_5k['gm_country_tran'], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False).sum()\n",
    "total_funds_recip = df_over_5k.pivot_table(index=df_over_5k['recip_country_tran'], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Top 10 funders make up {:.2f}% of total funding globally'.format((total_top_10_gm/total_funds_gm)*100))\n",
    "print('Top 10 recipients recieve {:.2f}% of total funding globally'.format((total_top_10_recip/total_funds_recip)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"font-family:\tCourier New;\"> \n",
    "    Majority of grant makers and recipients are from the United States (peaking at around 130,000). Many of the top 10 funders and recipients include European countries, with United Kingdom being the with the second highest amount of grant makers and recipients. Funders based in devloping countries fall in the top 10 funders (Ghana, Kenya and Brazil) and recipients (SA, Brazil, Kenya and India).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **What type of projects in these type of regions (europe/u.s./developing)?**\n",
    "\n",
    "- heatmap on grantmaker countries and recipient and sum of amounts? or exclude america "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Top Funders and Recipients Cities (Global)- Number of Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(9,4))\n",
    "# plt.figure(figsize=(8,5))\n",
    "# df_over_5k_england['recip_city'].value_counts()[df_over_5k_england['recip_city'].value_counts()>= 10].plot(kind='barh')\n",
    "\n",
    "df_over_5k['recip_city'].value_counts()[:20].plot(kind='barh', ax=ax[0])\n",
    "df_over_5k_england['recip_city'].value_counts()[:20].plot(kind='barh', ax=ax[1])\n",
    "\n",
    "ax[0].set_xlabel('Number of Projects')\n",
    "ax[0].set_ylabel('City', fontsize=10)\n",
    "ax[0].set_title('Top 10 cities with the most number of projects', fontsize=10)\n",
    "ax[0].grid(alpha=0.3)\n",
    "\n",
    "ax[1].set_xlabel('Number of Projects')\n",
    "ax[1].set_ylabel('City', fontsize=10)\n",
    "ax[1].set_title('Top 10 UK cities with most number of projects', fontsize=10)\n",
    "ax[1].grid(alpha=0.3)\n",
    "\n",
    "ax[0].grid(alpha=0.2)\n",
    "ax[0].invert_yaxis()\n",
    "\n",
    "ax[1].grid(alpha=0.2)\n",
    "ax[1].invert_yaxis()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Top Funders and Recipients Cities (Global)- Amount of Funding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(9,4))\n",
    "# plt.figure(figsize=(8,5))\n",
    "# df_over_5k_england['recip_city'].value_counts()[df_over_5k_england['recip_city'].value_counts()>= 10].plot(kind='barh')\n",
    "\n",
    "df_over_5k.pivot_table(index=df_over_5k[['recip_city']], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False)[:20].plot(kind='barh', fontsize=9, ax=ax[0], logx=True)\n",
    "df_over_5k_gb.pivot_table(index=df_over_5k[['recip_city']], columns ='yr_issued', aggfunc=np.sum)['amount'].sum(axis=1).sort_values(ascending=False)[:20].plot(kind='barh', fontsize=9, ax=ax[1], logx=True)\n",
    "\n",
    "ax[0].set_xlabel('Amount of funding received over the years')\n",
    "ax[0].set_ylabel('City', fontsize=10)\n",
    "ax[0].set_title('Top 10 cities with most total amount of funding received', fontsize=10)\n",
    "ax[0].grid(alpha=0.3)\n",
    "\n",
    "ax[1].set_xlabel('Amount of funding received over the years')\n",
    "ax[1].set_ylabel('City', fontsize=10)\n",
    "ax[1].set_title('Top 10 UK cities with most total amount of funding received', fontsize=10)\n",
    "ax[1].grid(alpha=0.3)\n",
    "\n",
    "ax[0].grid(alpha=0.2)\n",
    "ax[0].invert_yaxis()\n",
    "\n",
    "ax[1].grid(alpha=0.2)\n",
    "ax[1].invert_yaxis()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Media Projects over Time*/*UK Media Projects against All Media projects#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10,6), sharex=True)\n",
    "\n",
    "# media_year_counts = df_over_5k_media.groupby('yr_issued')['amount'].count()\n",
    "all_year_counts = df_over_5k.groupby('yr_issued')['amount'].count()\n",
    "all_year_counts_eng = df_over_5k_gb.groupby('yr_issued')['amount'].count()\n",
    "\n",
    "# media_year_counts.plot(marker='o', ax=ax[0,0])\n",
    "# (media_year_counts/all_year_counts).plot(marker='o', ax=ax[0,1])\n",
    "all_year_counts.plot(ax=ax[0])#marker='o', \n",
    "all_year_counts_eng.plot(color='orange', ax=ax[0]) #marker='o', \n",
    "(all_year_counts_eng/all_year_counts).plot(ax=ax[1], color='green') #marker='o', \n",
    "\n",
    "ax[0].set_xlabel('Year Issued')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[0].set_title('Number of Grants')\n",
    "ax[0].legend(['Grants (All)','UK Grants'])\n",
    "ax[0].grid(alpha=0.3)\n",
    "\n",
    "ax[1].set_xlabel('Year Issued')\n",
    "ax[1].set_ylabel('Fraction')\n",
    "ax[1].set_title('Fraction of Grants received by UK grantees');\n",
    "ax[1].grid(alpha=0.3)\n",
    "\n",
    "# media_england_year_counts = df_over_5k_england_media.groupby('yr_issued')['amount'].count()\n",
    "# england_year_counts = df_over_5k_england.groupby('yr_issued')['amount'].count()\n",
    "\n",
    "# media_england_year_counts.plot(marker='o', ax=ax[1,0], color='green')\n",
    "# (media_england_year_counts/england_year_counts).plot(marker='o', ax=ax[1,1], color='green')\n",
    "\n",
    "# ax[1,0].set_xlabel('Year Issued')\n",
    "# ax[1,0].set_ylabel('Frequency')\n",
    "# ax[1,0].set_title('UK Media Grants')\n",
    "# ax[1,0].grid(alpha=0.3)\n",
    "\n",
    "# ax[1,1].set_xlabel('Year Issued')\n",
    "# ax[1,1].set_ylabel('Fraction')\n",
    "# ax[1,1].set_title('Fraction of UK Grants that are Media');\n",
    "# ax[1,1].grid(alpha=0.3)\n",
    "ax[0].set_ylim(0,24000)\n",
    "ax[1].set_ylim(0, 0.05)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"font-family:\tCourier New;\">The number of projects increase over time 2015, slight decline up to 2017 but sharp decline afterwards. Maybe because of errors in entering projects, missing data or projects which have yet not started. Number of media projects spike in 2015 but have the lowest fraction in terms of all projects in the same year (increase in projects?). Rise in fraction of media projects from 2017 to 2019 is mostly due to noise since not many grants issued that year.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"font-family:\tCourier New;\">As expected, the number of UK grants, including media grants, is siginificantly less than the total amount. The number of UK media projects sharply peaked in 2015, however did slightly fall in amount as it approached 2015. From here onwards, there was a sharp drop in media grant numbers but did stagnate from 2016-2017. The fraction of media projects across fall in the 50%-60% range for most years. This is true even when the amount of projects fluctuate.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Dips- lag in reporting for 2018-2020 porjs or real decline? how much by (lag)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Year issued- date of decision or date of receipt (money given)?** \n",
    "\n",
    "- **Filtered beforehand? at least 40% of projects are media related**\n",
    "\n",
    "- **Yearly installments on the same project?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: Funding Amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Descriptive Stats over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Funding Distribution (Full Media Dataset & UK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10,3))\n",
    "\n",
    "log_amount = np.log10(df_over_5k['amount'])\n",
    "log_amount.plot(kind='hist', bins=50, ax=ax[0])\n",
    "ax[0].set_xlabel('Funding Amount USD (log10)')\n",
    "ax[0].axvline(log_amount.mean(), linestyle='--', color='black', label='mean: $10^{{{}}}$'.format(log_amount.mean()))\n",
    "ax[0].axvline(log_amount.quantile(0.5), linestyle='--', color='orange', label='median')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Funding Amounts per Project- Global')\n",
    "ax[0].set_xlabel('Funding Amount (log10)')\n",
    "ax[0].set_ylabel('Number of Projects', fontsize=10)\n",
    "ax[0].set_xticks(np.arange(3.5, 8.5, 0.5))\n",
    "\n",
    "log_amount = np.log10(df_over_5k_gb['amount'])\n",
    "log_amount.plot(kind='hist', bins=50, ax=ax[1])\n",
    "ax[1].set_xlabel('Funding Amount USD (log10)')\n",
    "ax[1].axvline(log_amount.mean(), linestyle='--', color='black', label='mean: $10^{{{}}}$'.format(log_amount.mean()))\n",
    "ax[1].axvline(log_amount.quantile(0.5), linestyle='--', color='orange', label='median')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('Funding Amounts per Project- UK')\n",
    "ax[1].set_xlabel('Funding Amount (log10)')\n",
    "ax[1].set_ylabel('Number of Projects', fontsize=10)\n",
    "ax[1].set_xticks(np.arange(3.5, 8.5, 0.5))\n",
    "\n",
    "print('Mean Amount: ${:.2f}'.format(df_over_5k_media['amount'].mean()))\n",
    "print('Median Amount: ${:.2f}'.format(df_over_5k_media['amount'].quantile(0.5)))\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../reports/figures/distrib_total_amount_global_uk.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"font-family:\tCourier New;\">The distribution of the amount seems to be right-tailed in the lower end of the funding issued (As seen in the log plot). Evident in the median funding falling below the mean. Downward slope of projects tend to decrease as the funding amount increases. Spikes seem to happen around round numbers (10K, 100K, etc). Heighest number of projects fall below the 10K mark.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"font-family: Courier New;\">\n",
    "    Many projects (approx. 600) fall under the \\$1 million mark, with a a peak of approximately 525 projects with grants around the \\$20K (10^4.3) mark. One project with a \\$10 million grant.\n",
    "    The median is to the left of the which indicates that this is right-tailed distribution (many projects with lower grants). \n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Median is higher than the gloabl media- implies the UK projects are getting more than the global average and interesting since most global prjects are from the U.S. Doe sthis indicate most UK recips are awarded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8,5))\n",
    "# # plt.hist(df_over_5k_media['amount'], bins=50, log=True,density='normed', histtype='step',alpha=0.5) # cumulative=True,\n",
    "# plt.hist(df_over_5k_england_media['amount'], bins=100, log=True, density='normed', histtype='step',alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Funding over time (descriptive analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "###ignore 2018 onwards since great drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fund_amount_time = df_over_5k.groupby('yr_issued')['amount'].describe()\n",
    "fund_amount_time_gb = df_over_5k_gb.groupby('yr_issued')['amount'].describe()#[['25%','50%','mean','75%']]\n",
    "\n",
    "fund_amount_time_gb.loc[:2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fund_amount_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fund_amount_time.loc[:2018][['25%','50%','mean','75%']].plot(marker='o', cmap='tab20',grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(12,4))#\n",
    "\n",
    "fund_amount_time.loc[:2017][['25%','50%','mean','75%']].plot(cmap='tab20',ax=ax[0])\n",
    "fund_amount_time_gb.loc[:2017][['25%','50%','mean','75%']].plot(cmap='tab20',ax=ax[1])\n",
    "# fund_amount_time_eng_media[['25%','50%','mean','75%']].plot(marker='o', cmap='viridis', ax=ax[1], grid=True)\n",
    "\n",
    "ax[0].set_ylim(0, 400000)\n",
    "ax[1].set_ylim(0, 400000)\n",
    "\n",
    "# ax[0].get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n",
    "# ax[1].get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n",
    "ax[0].set_xlabel('Year Issued')\n",
    "ax[0].set_ylabel('Value (USD)')\n",
    "ax[0].set_title('Total Amount of Funding Received Gloablly')\n",
    "\n",
    "ax[1].set_xlabel('Year Issued')\n",
    "ax[1].set_ylabel('Value (USD)')\n",
    "ax[1].set_title('Total Amount of Funding Received in the UK')\n",
    "\n",
    "# ax[1].set_xlabel('Year Issued')\n",
    "# ax[1].set_ylabel('Value (USD)')\n",
    "# ax[1].set_title('Funds issued to Media Grants in the UK')\n",
    "\n",
    "print('Maximum total sum amount- All UK projects:')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../reports/figures/total_amount_recieved_years_stats_global_uk.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"font-family: Courier New;\">Stopped at 2018. The mean both cases tend to be higher than the quantiles. This must be dragged by the maximum values (bias). Median a better view. Spike of aggregated funding in 2017 for all projects in england, spike in 2013 for english media projects.\n",
    "Median and lower quartile are simialr in both cases.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Why dip in global and stay stagnant? \n",
    "- less consist in big grants year to year in UK (the volatility in the mean reflects this)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Median Funding over Time per Country (U.K. v.s. Rest of Top 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_over_5k['recip_country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fund_amount_time_us = df_over_5k[df_over_5k['recip_country']=='US'].groupby('yr_issued')['amount'].median()\n",
    "fund_amount_time_gb = df_over_5k_gb.groupby('yr_issued')['amount'].median()\n",
    "fund_amount_time_za = df_over_5k[df_over_5k['recip_country']=='ZA'].groupby('yr_issued')['amount'].median()\n",
    "fund_amount_time_nl = df_over_5k[df_over_5k['recip_country']=='NL'].groupby('yr_issued')['amount'].median()\n",
    "fund_amount_time_in = df_over_5k[df_over_5k['recip_country']=='IN'].groupby('yr_issued')['amount'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_df_top_5 = pd.DataFrame({'US': fund_amount_time_us, 'GB': fund_amount_time_gb,\n",
    "             'ZA': fund_amount_time_za, 'NL': fund_amount_time_nl,\n",
    "             'IN': fund_amount_time_in}).loc[:2017]\n",
    "time_df_top_5.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_df_top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ax=plt.plot( 'yr_issued', 'US', data=time_df_top_5, marker='', color='skyblue', linewidth=2, alpha=0.6)\n",
    "# ax=plt.plot( 'yr_issued', 'GB', data=time_df_top_5, marker='', color='crimson', linewidth=4)\n",
    "# ax=plt.plot( 'yr_issued', 'ZA', data=time_df_top_5, marker='', color='olivedrab', linewidth=2, label=\"ZA\", alpha=0.6)\n",
    "# ax=plt.plot( 'yr_issued', 'NL', data=time_df_top_5, marker='', color='blue', linewidth=2, label=\"NL\", alpha=0.6)\n",
    "# ax=plt.plot( 'yr_issued', 'IN', data=time_df_top_5, marker='', color='darkorange', linewidth=2, label=\"IN\", alpha=0.6)\n",
    "# plt\n",
    "ax= time_df_top_5.plot(x='yr_issued', y = ['US', 'GB', 'ZA', 'NL', 'IN'], \n",
    "                       legend=False, figsize=(8,7))\n",
    "# plt.yscale('log')\n",
    "\n",
    "for line, name in zip(ax.lines, time_df_top_5[['US', 'GB', 'ZA', 'NL', 'IN']].columns):\n",
    "    y = line.get_ydata()[-1]\n",
    "    ax.annotate(name, xy=(1,y), xytext=(6,0.3), color=line.get_color(), \n",
    "                xycoords = ax.get_yaxis_transform(), textcoords=\"offset points\",\n",
    "                size=14, va=\"center\")\n",
    "plt.title('Total Median Funding Received Over Time')\n",
    "plt.ylabel('Median Amount', fontsize=12)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "# plt.legend()\n",
    "plt.savefig('../../reports/figures/distrib_total_amount_median_top5_and_uk.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Global Funding Distribution (GM to Recipient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df_over_5k_media['gm_country'].value_counts(dropna=False)\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "agg_piv = df_over_5k.pivot_table(index=df_over_5k_media['gm_country'], columns ='recip_country', aggfunc=np.sum)['amount']\n",
    "sns.heatmap(np.log10(agg_piv), square=True, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# agg_piv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = df_over_5k.pivot_table(index=df_over_5k[['gm_country','recip_country']], columns ='yr_issued', aggfunc=np.sum)['amount']\n",
    "t_2 = t.loc[~(t.index.get_level_values('gm_country') == 'US') |~(t.index.get_level_values('recip_country') == 'US')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "WHAT TO DO NEXT:\n",
    "Drop all the NAN Values- dont't fill with 0 since this makes the network graph too crowded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t_uk = t.loc[(t.index.get_level_values('gm_country') == 'GB') | (t.index.get_level_values('recip_country') == 'GB')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# t_2=t.fillna(0)\n",
    "# t_uk = t_uk.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t_2.reset_index(inplace=True)\n",
    "t_uk.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# t_dict_df = pd.DataFrame(\n",
    "#     {\n",
    "#         'source': t_['gm_country'] + '_source',\n",
    "#         'target': t_['recip_country']+ '_target',\n",
    "#         'value': t_[2015]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# t_uk_dict_df = pd.DataFrame(\n",
    "#     {\n",
    "#         'source': t_uk['gm_country'] + '_source',\n",
    "#         'target': t_uk['recip_country'] + '_target',\n",
    "#         'value': t_uk[2015]\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# g = nx.from_pandas_edgelist(t_dict_df, 'source', 'target', edge_attr= 'value')\n",
    "# g_uk = nx.from_pandas_edgelist(t_uk_dict_df, 'source', 'target', edge_attr= 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# layout_uk = nx.spring_layout(g_uk,iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# d_uk = dict(g_uk.degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# nx.draw_networkx_nodes(g_uk, \n",
    "#                        layout,\n",
    "#                        nodelist= d_uk.keys(),\n",
    "#                        node_size=[v*50 for v in d_uk.values()], \n",
    "#                        node_color='lightblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# nx.draw_networkx(g_uk,\n",
    "#                 layout,\n",
    "#                 nodelist= d_uk.keys(),\n",
    "#                 node_size=[v*50 for v in d_uk.values()], \n",
    "#                 node_color='lightblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# t_[2013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "g_ = nx.convert_matrix.from_pandas_edgelist(t_2, source = 'gm_country',\n",
    "                                       target = 'recip_country',\n",
    "                                      edge_attr=2014, create_using=nx.DiGraph())\n",
    "\n",
    "layout_ = nx.random_layout(g_)\n",
    "\n",
    "layout_ = nx.random_layout(g_)\n",
    "\n",
    "edges, weights = zip(*nx.get_edge_attributes(g_, 2014).items())\n",
    "d_ = dict(g_.degree())\n",
    "\n",
    "edges = nx.draw_networkx_edges(g_, layout_,\n",
    "                               node_size=900, arrowstyle='->',\n",
    "                               arrowsize=20, #edge_color='b',\n",
    "                               edgelist = edges, edge_color='black',\n",
    "                               edge_cmap=plt.cm.Blues, alpha=0.2,\n",
    "                              width=3)\n",
    "nodes = nx.draw_networkx_nodes(g_, layout_,\n",
    "                               node_size=900,\n",
    "                               node_color=[d[1] for d in g_.in_degree()],\n",
    "                               cmap= plt.cm.tab20c,\n",
    "                               with_labels=True,\n",
    "                               arrows=True,\n",
    "                               arrow_size=True,\n",
    "                               width=2,\n",
    "                              alpha=0.7)\n",
    "nodes=nx.draw_networkx_nodes(g_, layout_,\n",
    "                               node_size=900,\n",
    "                               node_color=[d[1] for d in g_.in_degree()], \n",
    "                               cmap= plt.cm.tab20c,\n",
    "                               arrows=True,\n",
    "                               arrow_size=True,\n",
    "                               width=2,\n",
    "                            alpha=0.7)\n",
    "labels=nx.draw_networkx_labels(g_, layout_, font_size=15, font_weight='bold')\n",
    "plt.colorbar(nodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from networkx.algorithms import community\n",
    "\n",
    "communities_generator = community.girvan_newman(g_)\n",
    "top_level_communities = next(communities_generator)\n",
    "next_level_communities = next(communities_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sorted(map(sorted, next_level_communities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plt.colormaps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# list(nx.selfloop_edges(g_uk_))\n",
    "\n",
    "# colors = [g_uk_[u][v][2015] for u,v in edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "g_uk = nx.convert_matrix.from_pandas_edgelist(t_uk, source = 'gm_country',\n",
    "                                       target = 'recip_country',\n",
    "                                      edge_attr=2015, create_using=nx.DiGraph())\n",
    "\n",
    "layout_ = nx.random_layout(g_uk)\n",
    "\n",
    "layout_ = nx.random_layout(g_uk)\n",
    "\n",
    "edges, weights = zip(*nx.get_edge_attributes(g_uk, 2015).items())\n",
    "d_ = dict(g_uk.degree())\n",
    "\n",
    "edges = nx.draw_networkx_edges(g_uk, layout_,\n",
    "                               node_size=900, arrowstyle='->',\n",
    "                               arrowsize=20, #edge_color='b',\n",
    "                               edgelist = edges, edge_color='black',\n",
    "                               edge_cmap=plt.cm.Blues, alpha=0.2,\n",
    "                              width=3)\n",
    "nodes = nx.draw_networkx_nodes(g_uk, layout_,\n",
    "                               node_size=900,\n",
    "                               node_color=[d[1] for d in g_uk.out_degree()],\n",
    "                               cmap= plt.cm.tab20c,\n",
    "                               with_labels=True,\n",
    "                               arrows=True,\n",
    "                               arrow_size=True,\n",
    "                               width=2,\n",
    "                              alpha=0.7)\n",
    "nodes=nx.draw_networkx_nodes(g_uk, layout_,\n",
    "                               node_size=900,\n",
    "                               node_color=[d[1] for d in g_uk.out_degree()], \n",
    "                               cmap= plt.cm.tab20c,\n",
    "                               arrows=True,\n",
    "                               arrow_size=True,\n",
    "                               width=2,\n",
    "                            alpha=0.7)\n",
    "labels=nx.draw_networkx_labels(g_uk, layout_, font_size=15, font_weight='bold')\n",
    "plt.colorbar(nodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# g_uk.edges()\n",
    "# nx.get_edge_attributes(g_uk,2015)\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "m = cm.ScalarMappable(norm=norm_, cmap=plt.cm.Blues)\n",
    "\n",
    "[norm_(i) for i in nx.get_edge_attributes(g_uk,2015).values()]\n",
    "# m.to_rgba(nx.get_edge_attributes(g_uk,2015))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[d[1] for d in g_.in_degree()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g_uk_.attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_uk = dict(g_uk_.degree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layout_uk = nx.spring_layout(g_uk,iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,10))\n",
    "# nx.draw_networkx(g_uk,\n",
    "#                 layout_uk,\n",
    "#                 nodelist= d_uk.keys(),\n",
    "#                 node_size=20, \n",
    "#                 node_color='lightblue', \n",
    "#                 edge_color = 'gray',\n",
    "#                 font_size = 8,\n",
    "#                 arrows=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_over_5k[df_over_5k['recip_country'] == 'GB']['recip_city'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(ncols=2,figsize = (10,5))\n",
    "# df_over_5k_media.groupby('yr_issued')['amount'].mean().plot(kind='bar', ax=ax[0])\n",
    "# ax1 = fig.add_subplot(111)\n",
    "# df_over_5k[df_over_5k['is_media']==False].groupby('yr_issued')['amount'].mean().plot(kind='bar', ax=ax1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plt.scatter(df_over_5k[(np.abs(stats.zscore(df_over_5k['amount'])) < 70)].groupby('yr_issued')['amount'].index,\n",
    "#             df_over_5k[(np.abs(stats.zscore(df_over_5k['amount'])) < 70)].groupby('yr_issued')['amount'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_over_5k[(np.abs(stats.zscore(df_over_5k['amount'])) < 70)].groupby('yr_issued')['amount'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.abs(stats.zscore(df_over_5k['amount'])>).tolist()#.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df_over_5k['amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df_over_5k_england_media.groupby('yr_issued')['amount'].\n",
    "# df_over_5k_england_media['amount'].plot(kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_over_5k['over_100char'] = df_over_5k['description'].str.len() > 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_over_5k['dup']=df_over_5k['description'].apply(lambda x: x.lower() if type(x) == str else x).duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df_over_5k['description'][]\n",
    "df_media = df_over_5k[(df_over_5k['over_100char'] == True) & (df_over_5k['dup'] ==False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_media['description'][:10][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Funding Intervals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_over_5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dup = df_over_5k[df_over_5k['grant_key'].duplicated(False)]#.loc[[1,2]]#.T\n",
    "dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dup['grant_key'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dup[dup['grant_key'] == 20464956]#['yr_issued']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for col in df_over_5k.columns:\n",
    "#     print(col)\n",
    "    print(dup[dup['grant_key'] == 20464956][col].duplicated(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# dup.loc[102408]\n",
    "dup[dup['grant_key'] == 20464956]['source_file_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dup[~dup['fund_type'].isnull()]['fund_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "col_pairs = list(itertools.combinations(df_over_5k.columns,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for col in df_over_5k.columns:\n",
    "#     print(col)\n",
    "    print(dup[col].duplicated(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dup['source_file_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"font-family: Courier New;\">Seems like there are a few duplicates in this dataset which are not picked since there may be a slight discrepency in one of the duplicates. Approx. 1689 dups.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: Geomaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_gdf = gpd.read_file('../../data/raw/NUTS_RG_01M_2016_4326_LEVL_1.shp/NUTS_RG_01M_2016_4326_LEVL_1.shp')\n",
    "# points_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['lon'], df['lat']))\n",
    "# points_in_poly = gpd.sjoin(points_gdf, poly_gdf, op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_uk = poly_gdf[poly_gdf['CNTR_CODE']=='UK']\n",
    "\n",
    "# poly_uk.rename(columns={'geometry':'geo_poly'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_5k_gb['city_country'] = df_over_5k_gb['recip_city'] + ', ' + df_over_5k_gb['recip_country_tran']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using google geocoding to get lat lon of cities in U.K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.read('../../.env')\n",
    "api_key = config['geocode']['api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_5k_gb['city_country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://maps.googleapis.com/maps/api/geocode/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode(address):\n",
    "    params={'address': address, 'key': api_key}\n",
    "    headers = {}\n",
    "    url='https://maps.googleapis.com/maps/api/geocode/json'\n",
    "    r = requests.get(url=url, params=params, headers=headers)\n",
    "    if r.status_code == 200:\n",
    "        \n",
    "    \n",
    "        return r.json()['results'][0]['geometry']['location']\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lat_lon_map = {}\n",
    "for i in df_over_5k_gb['city_country'].unique():\n",
    "    lat_lon_map[i] = geocode(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpacking the dicts so can easily map\n",
    "lat_dict ={}\n",
    "for i,k in lat_lon_map.items():\n",
    "    lat_dict[i] = k['lat']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_dict ={}\n",
    "for i,k in lat_lon_map.items():\n",
    "    lon_dict[i] = k['lng']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/interim/lat_dict.json','w') as f:\n",
    "    json.dump(lat_dict,f)\n",
    "    \n",
    "with open('../../data/interim/lon_dict.json','w') as f:\n",
    "    json.dump(lon_dict,f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/interim/lat_dict.json','r') as f:\n",
    "    lat_dict = json.load(f)\n",
    "    \n",
    "with open('../../data/interim/lon_dict.json','r') as f:\n",
    "    lon_dict = json.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_5k_gb['lat'] = df_over_5k_gb['city_country'].map(lat_dict)\n",
    "df_over_5k_gb['lon'] = df_over_5k_gb['city_country'].map(lon_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amount of projects with location info\n",
    "df_over_5k_gb['lon'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gdf = gpd.GeoDataFrame(df_over_5k_gb, geometry=gpd.points_from_xy(df_over_5k_gb['lon'], df_over_5k_gb['lat']))\n",
    "points_in_poly = gpd.sjoin(points_gdf, poly_uk, op='within', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = poly_uk[['NUTS_NAME', 'geometry']]\n",
    "x = x.sort_values(by='NUTS_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### mean & median amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean\n",
    "x['amount'] = points_in_poly[['NUTS_NAME', 'amount']].sort_values(by='NUTS_NAME').groupby('NUTS_NAME').mean()['amount'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.plot(column='amount', figsize = (8,10), legend=True).axis('off')\n",
    "# %%time\n",
    "# point_poly_amount_total = points_in_poly[['NUTS_NAME', 'geometry', 'amount']].dissolve(by='NUTS_NAME', aggfunc='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#median\n",
    "x['amount'] = points_in_poly[['NUTS_NAME', 'amount']].sort_values(by='NUTS_NAME').groupby('NUTS_NAME').median()['amount'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.plot(column='amount', figsize = (8,10), legend=True).axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### number of projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['count'] =points_in_poly[['NUTS_NAME', 'amount']].sort_values(by='NUTS_NAME').groupby('NUTS_NAME').count()['amount'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.plot(column='count', figsize = (8,10), legend=True).axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get descrips thta are at least over 100 chars and are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nesta.packages.nlp_utils.ngrammer import Ngrammer\n",
    "# ngrammer = Ngrammer(\"/mnt/c/Users/aotubusen/Documents/innovation-mapping.config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF doc-feature matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = 'hey how are you running!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english') +\n",
    "                 ['grant', 'general', 'project', 'support','new', 'give',\n",
    "                  'initiative', 'city', 'community', 'use', 'provide', 'information',\n",
    "                 'program', 'training', 'online', 'development', 'develop',\n",
    "                 'help', 'funding', 'fund']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_and_punc_remove(sentence):\n",
    "    p = ' '.join([contractions.get(i, i) for i in sentence.lower().split()])\n",
    "    \n",
    "    clean_tokens = [w for w in word_tokenize(p.lower()) if (w not in stop_words)]\n",
    "    clean_tokens_ = [re.sub(r'[^\\w\\s]','',w) for w in clean_tokens]\n",
    "    clean_tokens__ = [w for w in clean_tokens_ if len(w) > 1]\n",
    "    return clean_tokens__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_tokenizer(text):\n",
    "    results = []\n",
    "    for i in text:\n",
    "        lem = WordNetLemmatizer().lemmatize(i, pos='v')\n",
    "        results.append(lem)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenise_and_punc_remove(\"Adeola's hi.k bu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decided to stick to bigrams only since unigrams are simialr across the board and the trigrams that appear in top 20 features are not thta informative (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descrips_df = df_over_5k.loc[df_over_5k['description'].str.len() >= 100]\n",
    "descrips_df_gb = df_over_5k_gb.loc[df_over_5k_gb['description'].str.len() >= 100]#['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descrips_df_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descrips_df_unique = descrips_df.drop_duplicates(subset='description', keep='last')\n",
    "descrips_df_gb_unique = descrips_df_gb.drop_duplicates(subset='description', keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descrips_df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descrips_df_unique.reset_index(inplace=True, drop=True)\n",
    "descrips_df_gb_unique.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(descrips_df_gb_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "descrips_df_unique['tokens'] = descrips_df_unique['description'].apply(lambda x: lem_tokenizer(tokenise_and_punc_remove(x)))\n",
    "\n",
    "descrips_df_gb_unique['tokens'] = descrips_df_gb_unique['description'].apply(lambda x: lem_tokenizer(tokenise_and_punc_remove(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descrips_df_unique.to_csv('../../data/processed/descrips_df_unique_sent_df.csv', index=False)\n",
    "descrips_df_gb_unique.to_csv('../../data/processed/descrips_df_eng_unique_sent_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "descrips_df_unique_sent = pd.read_csv('../../data/processed/descrips_df_unique_sent_df.csv',converters={'tokens':literal_eval})\n",
    "descrips_df_gb_unique_sent = pd.read_csv('../../data/processed/descrips_df_eng_unique_sent_df.csv',converters={'tokens':literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descrips_df_eng_unique_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(descrips_df_unique_sent['tokens'], min_count=1, threshold=1)\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descrips_df_unique_bigrams = descrips_df_unique_sent.apply(lambda x: [i for i in bigram[x] if '_' in i])\n",
    "# descrips_df_eng_unique_bigrams = descrips_df_eng_unique_sent.apply(lambda x: [i for i in bigram[x] if '_' in i])\n",
    "\n",
    "descrips_df_unique_ngrams = descrips_df_unique_sent['tokens'].apply(lambda x: bigram[x])\n",
    "descrips_df_gb_unique_ngrams = descrips_df_gb_unique_sent['tokens'].apply(lambda x: bigram[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = descrips_df_unique.apply(lambda x: list(itertools.chain.from_iterable(ngrammer.process_document(x[0]))), axis=1)\n",
    "# tokens_eng = descrips_df_eng_unique.apply(lambda x: list(itertools.chain.from_iterable(ngrammer.process_document(x[0]))), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply TF-IDF to generate scores matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=identity_tokenizer,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        min_df=2,\n",
    "                       ngram_range=(2,2)) \n",
    "# tfidf_eng = TfidfVectorizer(tokenizer=identity_tokenizer,\n",
    "#                         stop_words=None,\n",
    "#                         lowercase=False,\n",
    "#                         min_df=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Doing `ngram_range` even AFTER bigram phraser because e.g. if terms `provide children` and `provide adults` are important contextually but unigram `provide` only made it past the significance test and not `adult` and `child` in the gensim phraser, we lose context of who and what is being provided. e.g. providing children may give context of education whilst providing adults maybe be resources for work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_features = tfidf.fit_transform(descrips_df_gb_unique_ngrams)\n",
    "# docs_features_eng = tfidf.fit_transform(descrips_df_eng_unique_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_features\n",
    "# docs_features_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_feat_df = pd.DataFrame(docs_features.todense())\n",
    "# doc_feat_eng_df = pd.DataFrame(docs_features_eng.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun to perserve ID from original data \n",
    "# update: already sorted out the perservation- just rerun and check indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA for feature creation for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.sklearn_api.ldamodel import LdaTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.dictionary.Dictionary(descrips_df_gb_unique_ngrams)\n",
    "corpus = [dictionary.doc2bow(text) for text in descrips_df_gb_unique_ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lda = LdaTransformer(num_topics=12, id2word=dictionary, minimum_probability=0)\n",
    "lda_doc_vecs = lda.fit_transform(corpus)\n",
    "#100 topics 100-104 cluster = elbow\n",
    "#30topics 29-31 clusters = elbow\n",
    "\n",
    "# transfor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_doc_vecs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Searching for optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "distortions = []\n",
    "K = range(1,25,2)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(lda_doc_vecs)\n",
    "    distortions.append(kmeanModel.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()\n",
    "## seems like the number of topics coincide with the number of clusters (case for number under 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defin no. of clusters\n",
    "# siluette score\n",
    "# explained method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cluster_results = KMeans(n_clusters=11).fit_predict(lda_doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(cluster_results)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##### get top 10 tfidf values for clusters\n",
    "cluster_0 = list(doc_feat_df.loc[np.where(cluster_results == 0)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "cluster_1 = list(doc_feat_df.loc[np.where(cluster_results == 1)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "cluster_2 = list(doc_feat_df.loc[np.where(cluster_results == 2)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "cluster_3 = list(doc_feat_df.loc[np.where(cluster_results == 3)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "cluster_4 = list(doc_feat_df.loc[np.where(cluster_results == 4)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "cluster_5 = list(doc_feat_df.loc[np.where(cluster_results == 5)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "cluster_6 = list(doc_feat_df.loc[np.where(cluster_results == 6)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "cluster_7 = list(doc_feat_df.loc[np.where(cluster_results == 7)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "cluster_8 = list(doc_feat_df.loc[np.where(cluster_results == 8)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "cluster_9 = list(doc_feat_df.loc[np.where(cluster_results == 9)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "cluster_10 = list(doc_feat_df.loc[np.where(cluster_results == 10)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_11 = list(doc_feat_df.loc[np.where(cluster_results == 11)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_12 = list(doc_feat_df.loc[np.where(cluster_results == 12)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_13 = list(doc_feat_df.loc[np.where(cluster_results == 13)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_14 = list(doc_feat_df.loc[np.where(cluster_results == 14)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_15 = list(doc_feat_df.loc[np.where(cluster_results == 15)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_16 = list(doc_feat_df.loc[np.where(cluster_results == 16)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_17 = list(doc_feat_df.loc[np.where(cluster_results == 17)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_18 = list(doc_feat_df.loc[np.where(cluster_results == 18)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_19 = list(doc_feat_df.loc[np.where(cluster_results == 19)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_20 = list(doc_feat_df.loc[np.where(cluster_results == 20)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_21 = list(doc_feat_df.loc[np.where(cluster_results == 21)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_22 = list(doc_feat_df.loc[np.where(cluster_results == 22)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_23 = list(doc_feat_df.loc[np.where(cluster_results == 23)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_24 = list(doc_feat_df.loc[np.where(cluster_results == 24)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_25 = list(doc_feat_df.loc[np.where(cluster_results == 25)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_26 = list(doc_feat_df.loc[np.where(cluster_results == 26)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_27 = list(doc_feat_df.loc[np.where(cluster_results == 27)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_28 = list(doc_feat_df.loc[np.where(cluster_results == 28)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_29 = list(doc_feat_df.loc[np.where(cluster_results == 29)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "# cluster_30 = list(doc_feat_df.loc[np.where(cluster_results == 30)[0]].sum().sort_values(ascending=False)[:10].index)\n",
    "\n",
    "# cluster_9 = list(doc_feat_df.loc[np.where(cluster_results == 9)[0]].sum().sort_values(ascending=False)[:10].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([tfidf.get_feature_names()[n] for n in cluster_0])\n",
    "print([tfidf.get_feature_names()[n] for n in cluster_1])\n",
    "print([tfidf.get_feature_names()[n] for n in cluster_2])\n",
    "print([tfidf.get_feature_names()[n] for n in cluster_3])\n",
    "print([tfidf.get_feature_names()[n] for n in cluster_4])\n",
    "print([tfidf.get_feature_names()[n] for n in cluster_5])\n",
    "print([tfidf.get_feature_names()[n] for n in cluster_6])\n",
    "print([tfidf.get_feature_names()[n] for n in cluster_7])\n",
    "print([tfidf.get_feature_names()[n] for n in cluster_8])\n",
    "print([tfidf.get_feature_names()[n] for n in cluster_9])\n",
    "print([tfidf.get_feature_names()[n] for n in cluster_10])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_11])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_12])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_13])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_14])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_15])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_16])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_17])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_18])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_19])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_20])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_21])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_22])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_23])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_24])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_25])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_26])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_27])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_28])\n",
    "# print([tfidf.get_feature_names()[n] for n in cluster_14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# svd = TruncatedSVD(n_components=9)\n",
    "tsne = TSNE()\n",
    "\n",
    "# svd_vecs = svd.fit_transform(lda_doc_vecs)\n",
    "# tsne_vecs = tsne.fit_transform(svd_vecs)\n",
    "\n",
    "tsne_vecs = tsne.fit_transform(lda_doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_doc_vecs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.gensim_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne_vecs[:, 0], tsne_vecs[:, 1], c=cluster_results, cmap='hsv', alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf and lda use the same input!\n",
    "#if tfidf topic words to describe clusters look rubbish, maybe just feed it unigrams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=0.8, min_samples=50).fit(lda_doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(clustering.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = NearestNeighbors()\n",
    "neigh.fit(lda_doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for j in tokens:\n",
    "    words.extend(j)\n",
    "word_count = Counter(words)\n",
    "top_words= Counter(words).most_common(20)\n",
    "\n",
    "words_eng = []\n",
    "for j in tokens_eng:\n",
    "    words_eng.extend(j)\n",
    "word_count = Counter(words_eng)\n",
    "top_england_words= Counter(words_eng).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10,6))\n",
    "ax[0].grid(alpha=0.2)\n",
    "ax[1].grid(alpha=0.2)\n",
    "\n",
    "ax[0].set_xlim(0, 10500)\n",
    "# axs[1].set_ylim(0, 10000)\n",
    "\n",
    "ax[0].barh(*zip(*top_words), color = 'c')\n",
    "ax[1].barh(*zip(*top_england_words), color = 'c')\n",
    "\n",
    "ax[0].set_title('Top 20 words in unique descriptions (Globally)')\n",
    "ax[1].set_title('Top 20 words in unique descriptions (UK)')\n",
    "\n",
    "ax[0].invert_yaxis()\n",
    "ax[1].invert_yaxis()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[names[n] for n in list(top_grams[:20])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[names[n] for n in list(top_grams[:20])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
